{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# News Modeling\n\nTopic modeling involves **extracting features from document terms** and using\nmathematical structures and frameworks like matrix factorization and SVD to generate **clusters or groups of terms** that are distinguishable from each other and these clusters of words form topics or concepts\n\nTopic modeling is a method for **unsupervised classification** of documents, similar to clustering on numeric data\n\nThese concepts can be used to interpret the main **themes** of a corpus and also make **semantic connections among words that co-occur together** frequently in various documents\n\nTopic modeling can help in the following areas:\n- discovering the **hidden themes** in the collection\n- **classifying** the documents into the discovered themes\n- using the classification to **organize/summarize/search** the documents\n\nFrameworks and algorithms to build topic models:\n- Latent semantic indexing\n- Latent Dirichlet allocation\n- Non-negative matrix factorization","metadata":{}},{"cell_type":"markdown","source":"## Latent Dirichlet Allocation (LDA)\nThe latent Dirichlet allocation (LDA) technique is a **generative probabilistic model** where each **document is assumed to have a combination of topics** similar to a probabilistic latent semantic indexing model\n\nIn simple words, the idea behind LDA is that of two folds:\n- each **document** can be described by a **distribution of topics**\n- each **topic** can be described by a **distribution of words**","metadata":{}},{"cell_type":"markdown","source":"### LDA Algorithm\n\n- 1. For each document, **randomly initialize each word to one of the K topics** (k is chosen beforehand)\n- 2. For each document D, go through each word w and compute:\n    - **P(T |D)** , which is a proportion of words in D assigned to topic T\n    - **P(W |T )** , which is a proportion of assignments to topic T over all documents having the word W\n- **Reassign word W with topic T** with probability P(T |D)Â´ P(W |T ) considering all other words and their topic assignments\n\n![LDA](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/LDA.png)","metadata":{}},{"cell_type":"markdown","source":"### Steps\n- Install the necessary library\n- Import the necessary libraries\n- Download the dataset\n- Load the dataset\n- Pre-process the dataset\n    - Stop words removal\n    - Email removal\n    - Non-alphabetic words removal\n    - Tokenize\n    - Lowercase\n    - BiGrams & TriGrams\n    - Lemmatization\n- Create a dictionary for the document\n- Filter low frequency words\n- Create an Index to word dictionary\n- Train the Topic Model\n- Predict on the dataset\n- Evaluate the Topic Model\n    - Model Perplexity\n    - Topic Coherence\n- Visualize the topics","metadata":{}},{"cell_type":"markdown","source":"### Install the necessary library","metadata":{}},{"cell_type":"code","source":"! pip install pyLDAvis gensim spacy","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:01.986785Z","iopub.execute_input":"2022-03-21T15:48:01.987305Z","iopub.status.idle":"2022-03-21T15:48:19.613454Z","shell.execute_reply.started":"2022-03-21T15:48:01.987269Z","shell.execute_reply":"2022-03-21T15:48:19.612567Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install re","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:19.615382Z","iopub.execute_input":"2022-03-21T15:48:19.615706Z","iopub.status.idle":"2022-03-21T15:48:21.320043Z","shell.execute_reply.started":"2022-03-21T15:48:19.615669Z","shell.execute_reply":"2022-03-21T15:48:21.319205Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import nltk\n! nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:21.323400Z","iopub.execute_input":"2022-03-21T15:48:21.323627Z","iopub.status.idle":"2022-03-21T15:48:23.203529Z","shell.execute_reply.started":"2022-03-21T15:48:21.323599Z","shell.execute_reply":"2022-03-21T15:48:23.202658Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Import the libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re, nltk, spacy, gensim\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom gensim.utils import simple_preprocess","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:23.207402Z","iopub.execute_input":"2022-03-21T15:48:23.208015Z","iopub.status.idle":"2022-03-21T15:48:32.324905Z","shell.execute_reply.started":"2022-03-21T15:48:23.207983Z","shell.execute_reply":"2022-03-21T15:48:32.323266Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:32.326316Z","iopub.execute_input":"2022-03-21T15:48:32.326581Z","iopub.status.idle":"2022-03-21T15:48:32.330315Z","shell.execute_reply.started":"2022-03-21T15:48:32.326546Z","shell.execute_reply":"2022-03-21T15:48:32.329649Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim.models import LdaModel\nfrom gensim.corpora import Dictionary\nfrom pprint import pprint\nimport pandas as pd\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora, models\nimport gensim\nimport re, nltk, spacy, gensim\n","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:32.331839Z","iopub.execute_input":"2022-03-21T15:48:32.332423Z","iopub.status.idle":"2022-03-21T15:48:32.340269Z","shell.execute_reply.started":"2022-03-21T15:48:32.332386Z","shell.execute_reply":"2022-03-21T15:48:32.339557Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:52:14.735355Z","iopub.execute_input":"2022-03-21T15:52:14.735641Z","iopub.status.idle":"2022-03-21T15:52:14.745859Z","shell.execute_reply.started":"2022-03-21T15:52:14.735611Z","shell.execute_reply":"2022-03-21T15:52:14.745121Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### Download the dataset\nDataset: https://raw.githubusercontent.com/subashgandyer/datasets/main/newsgroups.json\n\n#### 20-Newsgroups dataset\n- 11K newsgroups posts\n- 20 news topics","metadata":{}},{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/subashgandyer/datasets/main/newsgroups.json","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:32.341385Z","iopub.execute_input":"2022-03-21T15:48:32.341700Z","iopub.status.idle":"2022-03-21T15:48:34.295867Z","shell.execute_reply.started":"2022-03-21T15:48:32.341665Z","shell.execute_reply":"2022-03-21T15:48:34.294999Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Load the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_json(\"newsgroups.json\")","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.297442Z","iopub.execute_input":"2022-03-21T15:48:34.297722Z","iopub.status.idle":"2022-03-21T15:48:34.459080Z","shell.execute_reply.started":"2022-03-21T15:48:34.297686Z","shell.execute_reply":"2022-03-21T15:48:34.458367Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.460449Z","iopub.execute_input":"2022-03-21T15:48:34.460717Z","iopub.status.idle":"2022-03-21T15:48:34.477269Z","shell.execute_reply.started":"2022-03-21T15:48:34.460682Z","shell.execute_reply":"2022-03-21T15:48:34.475999Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess the data","metadata":{}},{"cell_type":"markdown","source":"### Email Removal","metadata":{}},{"cell_type":"code","source":"df = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.480317Z","iopub.execute_input":"2022-03-21T15:48:34.480517Z","iopub.status.idle":"2022-03-21T15:48:34.485260Z","shell.execute_reply.started":"2022-03-21T15:48:34.480483Z","shell.execute_reply":"2022-03-21T15:48:34.484360Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pprint(df[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.486848Z","iopub.execute_input":"2022-03-21T15:48:34.487363Z","iopub.status.idle":"2022-03-21T15:48:34.494175Z","shell.execute_reply.started":"2022-03-21T15:48:34.487308Z","shell.execute_reply":"2022-03-21T15:48:34.493391Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Newline Removal","metadata":{}},{"cell_type":"code","source":"df = [re.sub('\\s+', ' ', sent) for sent in df]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.495635Z","iopub.execute_input":"2022-03-21T15:48:34.496506Z","iopub.status.idle":"2022-03-21T15:48:34.503331Z","shell.execute_reply.started":"2022-03-21T15:48:34.496469Z","shell.execute_reply":"2022-03-21T15:48:34.502598Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Single Quotes Removal","metadata":{}},{"cell_type":"code","source":"df = [re.sub(\"\\'\", \"\", sent) for sent in df]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.504565Z","iopub.execute_input":"2022-03-21T15:48:34.504918Z","iopub.status.idle":"2022-03-21T15:48:34.510436Z","shell.execute_reply.started":"2022-03-21T15:48:34.504880Z","shell.execute_reply":"2022-03-21T15:48:34.509694Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pprint(df[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.512042Z","iopub.execute_input":"2022-03-21T15:48:34.512369Z","iopub.status.idle":"2022-03-21T15:48:34.519230Z","shell.execute_reply.started":"2022-03-21T15:48:34.512335Z","shell.execute_reply":"2022-03-21T15:48:34.518300Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize\n- Create **sent_to_words()** \n    - Use **gensim.utils.simple_preprocess**\n    - Use **generator** instead of an usual function","metadata":{}},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndf_words = list(sent_to_words(df))\n\nprint(df_words[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.520739Z","iopub.execute_input":"2022-03-21T15:48:34.521056Z","iopub.status.idle":"2022-03-21T15:48:34.529041Z","shell.execute_reply.started":"2022-03-21T15:48:34.521022Z","shell.execute_reply":"2022-03-21T15:48:34.528277Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stop words Removal\n- Extend the stop words corpus with the following words\n    - from\n    - subject\n    - re\n    - edu\n    - use","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.531813Z","iopub.execute_input":"2022-03-21T15:48:34.532141Z","iopub.status.idle":"2022-03-21T15:48:34.542895Z","shell.execute_reply.started":"2022-03-21T15:48:34.532108Z","shell.execute_reply":"2022-03-21T15:48:34.542157Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_nostop = [raw for raw in df_words if not raw in stop_words]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.544303Z","iopub.execute_input":"2022-03-21T15:48:34.544806Z","iopub.status.idle":"2022-03-21T15:48:34.548772Z","shell.execute_reply.started":"2022-03-21T15:48:34.544765Z","shell.execute_reply":"2022-03-21T15:48:34.548026Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(df_nostop[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.550318Z","iopub.execute_input":"2022-03-21T15:48:34.550833Z","iopub.status.idle":"2022-03-21T15:48:34.557343Z","shell.execute_reply.started":"2022-03-21T15:48:34.550796Z","shell.execute_reply":"2022-03-21T15:48:34.556386Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### remove_stopwords( )","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(texts):\n    return[raw for raw in df_words if not raw in stop_words]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.558735Z","iopub.execute_input":"2022-03-21T15:48:34.559240Z","iopub.status.idle":"2022-03-21T15:48:34.565031Z","shell.execute_reply.started":"2022-03-21T15:48:34.559204Z","shell.execute_reply":"2022-03-21T15:48:34.564180Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_words_nostop = remove_stopwords(df_words)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.566575Z","iopub.execute_input":"2022-03-21T15:48:34.567108Z","iopub.status.idle":"2022-03-21T15:48:34.576935Z","shell.execute_reply.started":"2022-03-21T15:48:34.567042Z","shell.execute_reply":"2022-03-21T15:48:34.576178Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(df_words_nostop[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.578113Z","iopub.execute_input":"2022-03-21T15:48:34.578567Z","iopub.status.idle":"2022-03-21T15:48:34.586188Z","shell.execute_reply.started":"2022-03-21T15:48:34.578530Z","shell.execute_reply":"2022-03-21T15:48:34.585351Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Bigrams\n- Use **gensim.models.Phrases**\n- 100 as threshold","metadata":{}},{"cell_type":"code","source":"bigram = gensim.models.Phrases(df_words_nostop, min_count=5, threshold=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.587838Z","iopub.execute_input":"2022-03-21T15:48:34.588090Z","iopub.status.idle":"2022-03-21T15:48:34.595531Z","shell.execute_reply.started":"2022-03-21T15:48:34.588058Z","shell.execute_reply":"2022-03-21T15:48:34.594759Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### make_bigrams( )","metadata":{}},{"cell_type":"code","source":"def make_bigrams(texts):\n    return [bigram[doc] for doc in texts]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.597074Z","iopub.execute_input":"2022-03-21T15:48:34.597410Z","iopub.status.idle":"2022-03-21T15:48:34.603726Z","shell.execute_reply.started":"2022-03-21T15:48:34.597376Z","shell.execute_reply":"2022-03-21T15:48:34.603091Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_nostop_bigrams = make_bigrams(df_words_nostop)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.605027Z","iopub.execute_input":"2022-03-21T15:48:34.605431Z","iopub.status.idle":"2022-03-21T15:48:34.610953Z","shell.execute_reply.started":"2022-03-21T15:48:34.605397Z","shell.execute_reply":"2022-03-21T15:48:34.610143Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Lemmatization\n- Use spacy\n    - Download spacy en model (if you have not done that before)\n    - Load the spacy model","metadata":{}},{"cell_type":"code","source":"! python -m spacy download en","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:34.612327Z","iopub.execute_input":"2022-03-21T15:48:34.612625Z","iopub.status.idle":"2022-03-21T15:48:51.507057Z","shell.execute_reply.started":"2022-03-21T15:48:34.612592Z","shell.execute_reply":"2022-03-21T15:48:51.506182Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:51.510374Z","iopub.execute_input":"2022-03-21T15:48:51.510607Z","iopub.status.idle":"2022-03-21T15:48:52.478619Z","shell.execute_reply.started":"2022-03-21T15:48:51.510577Z","shell.execute_reply":"2022-03-21T15:48:52.477835Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### lemmatizaton( )","metadata":{}},{"cell_type":"code","source":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:52.480011Z","iopub.execute_input":"2022-03-21T15:48:52.480290Z","iopub.status.idle":"2022-03-21T15:48:52.486906Z","shell.execute_reply.started":"2022-03-21T15:48:52.480246Z","shell.execute_reply":"2022-03-21T15:48:52.485141Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"data_lemmatized = lemmatization(df_nostop_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:52.491691Z","iopub.execute_input":"2022-03-21T15:48:52.492108Z","iopub.status.idle":"2022-03-21T15:48:52.513160Z","shell.execute_reply.started":"2022-03-21T15:48:52.492079Z","shell.execute_reply":"2022-03-21T15:48:52.512477Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(data_lemmatized[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:52.514749Z","iopub.execute_input":"2022-03-21T15:48:52.515228Z","iopub.status.idle":"2022-03-21T15:48:52.519996Z","shell.execute_reply.started":"2022-03-21T15:48:52.515191Z","shell.execute_reply":"2022-03-21T15:48:52.518989Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Create a Dictionary","metadata":{}},{"cell_type":"code","source":"id2word = corpora.Dictionary(data_lemmatized)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:49:55.169539Z","iopub.execute_input":"2022-03-21T15:49:55.170284Z","iopub.status.idle":"2022-03-21T15:49:55.175755Z","shell.execute_reply.started":"2022-03-21T15:49:55.170244Z","shell.execute_reply":"2022-03-21T15:49:55.174968Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Create Corpus","metadata":{}},{"cell_type":"code","source":"texts = data_lemmatized","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:48:52.530120Z","iopub.execute_input":"2022-03-21T15:48:52.530409Z","iopub.status.idle":"2022-03-21T15:48:52.536347Z","shell.execute_reply.started":"2022-03-21T15:48:52.530374Z","shell.execute_reply":"2022-03-21T15:48:52.535652Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Filter low-frequency words","metadata":{}},{"cell_type":"code","source":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:50:54.334421Z","iopub.execute_input":"2022-03-21T15:50:54.335049Z","iopub.status.idle":"2022-03-21T15:50:54.340694Z","shell.execute_reply.started":"2022-03-21T15:50:54.335007Z","shell.execute_reply":"2022-03-21T15:50:54.340009Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Create Index 2 word dictionary","metadata":{}},{"cell_type":"code","source":"corpus = [id2word.doc2bow(text) for text in texts]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:50:15.704624Z","iopub.execute_input":"2022-03-21T15:50:15.704903Z","iopub.status.idle":"2022-03-21T15:50:15.709434Z","shell.execute_reply.started":"2022-03-21T15:50:15.704873Z","shell.execute_reply":"2022-03-21T15:50:15.708641Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"id2word[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:50:36.048609Z","iopub.execute_input":"2022-03-21T15:50:36.048884Z","iopub.status.idle":"2022-03-21T15:50:36.053927Z","shell.execute_reply.started":"2022-03-21T15:50:36.048855Z","shell.execute_reply":"2022-03-21T15:50:36.053284Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Build a News Topic Model\n\n#### LdaModel\n- **num_topics** : this is the number of topics you need to define beforehand\n- **chunksize** : the number of documents to be used in each training chunk\n- **alpha** : this is the hyperparameters that affect the sparsity of the topics\n- **passess** : total number of training assess","metadata":{}},{"cell_type":"code","source":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:51:11.016010Z","iopub.execute_input":"2022-03-21T15:51:11.016274Z","iopub.status.idle":"2022-03-21T15:51:11.046312Z","shell.execute_reply.started":"2022-03-21T15:51:11.016242Z","shell.execute_reply":"2022-03-21T15:51:11.045663Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Print the Keyword in the 10 topics","metadata":{}},{"cell_type":"code","source":"pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:51:19.147116Z","iopub.execute_input":"2022-03-21T15:51:19.147381Z","iopub.status.idle":"2022-03-21T15:51:19.157913Z","shell.execute_reply.started":"2022-03-21T15:51:19.147349Z","shell.execute_reply":"2022-03-21T15:51:19.157065Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of Topic Models\n- Model Perplexity\n- Topic Coherence\n\n### Model Perplexity\n\nModel perplexity is a measurement of **how well** a **probability distribution** or probability model **predicts a sample**","metadata":{}},{"cell_type":"code","source":"print('\\nPerplexity: ', lda_model.log_perplexity(corpus))","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:51:24.093387Z","iopub.execute_input":"2022-03-21T15:51:24.093982Z","iopub.status.idle":"2022-03-21T15:51:24.102529Z","shell.execute_reply.started":"2022-03-21T15:51:24.093943Z","shell.execute_reply":"2022-03-21T15:51:24.101802Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### Topic Coherence\nTopic Coherence measures score a single topic by measuring the **degree of semantic similarity** between **high scoring words** in the topic.","metadata":{}},{"cell_type":"code","source":"coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:51:26.257195Z","iopub.execute_input":"2022-03-21T15:51:26.257655Z","iopub.status.idle":"2022-03-21T15:51:26.299927Z","shell.execute_reply.started":"2022-03-21T15:51:26.257616Z","shell.execute_reply":"2022-03-21T15:51:26.299167Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the Topic Model\n- Use **pyLDAvis**\n    - designed to help users **interpret the topics** in a topic model that has been fit to a corpus of text data\n    - extracts information from a fitted LDA topic model to inform an interactive web-based visualization","metadata":{}},{"cell_type":"code","source":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds') \nvis","metadata":{"execution":{"iopub.status.busy":"2022-03-21T15:55:32.930064Z","iopub.execute_input":"2022-03-21T15:55:32.930556Z","iopub.status.idle":"2022-03-21T15:55:33.718724Z","shell.execute_reply.started":"2022-03-21T15:55:32.930499Z","shell.execute_reply":"2022-03-21T15:55:33.717871Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}